# -*- coding: utf-8 -*-
"""tes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pKxaMaerpC8R9z7XLee42gvGqtFSy09

### Import Library
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

!pip install scorecardpy
import scorecardpy as sc

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve
from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_validate
!pip install feature_engine
from feature_engine.selection import DropCorrelatedFeatures, SmartCorrelatedSelection

import warnings
warnings.filterwarnings("ignore")

sns.set(style='whitegrid')
sns.set_palette("bright")

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

"""### Load Data Set"""

apptes = pd.read_csv("/content/drive/MyDrive/application_test.csv")
apptes

apptrain = pd.read_csv("/content/drive/MyDrive/application_train.csv")
apptrain

bureau = pd.read_csv("/content/drive/MyDrive/bureau.csv")
bureau

prev = pd.read_csv("/content/drive/MyDrive/previous_application.csv")
prev

prev_app = prev.groupby(['SK_ID_CURR'])['SK_ID_CURR'].agg(['count']).reset_index()
prev_app.columns = ['SK_ID_CURR','TOTAL_PREV_APP']
prev_app

bureau_cred = bureau.groupby(['SK_ID_CURR'])['SK_ID_CURR'].agg(['count']).reset_index()
bureau_cred.columns = ['SK_ID_CURR','TOTAL_BUREAU_LOAN']
bureau_cred

df = (apptrain.merge(prev_app, how='left', on='SK_ID_CURR')).merge(bureau_cred, how='left', on='SK_ID_CURR')
df

"""### Data Cleaning"""

# Cek setiap kolom data untuk mengetahui kolom yg memiliki missing value diatas 30%
total = df.isnull().sum().sort_values(ascending = False)
percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)
missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).reset_index()
missing_data.loc[missing_data['Percent'] >30]

(missing_data.loc[missing_data['Percent'] >0]).shape

df.duplicated().sum()

df.dtypes.value_counts()

df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)

"""Data categorical diatas relatif kecil nilainya, jadi nanti akan dilakukan feature encoding untuk menyelesaikanya

### EDA
"""

print("0: will repay on time")
print("1: will have difficulty repaying loan")

class_dist = df['TARGET'].value_counts()

plt.figure(figsize=(12,3))
plt.title('Class Distribution')
plt.barh(class_dist.index, class_dist.values)
plt.yticks([0, 1])

for i, value in enumerate(class_dist.values):
    plt.text(value-2000, i, str(value), fontsize=12, color='white',
             horizontalalignment='right', verticalalignment='center')

plt.show()

"""- 1 : Bad Customer
- 2 : Good Customer
"""

plt.figure(1)
plt.subplot(221)
df['CODE_GENDER'].value_counts(normalize=True).plot.bar(figsize=(8,8), title= 'Client Gender', color=['red','blue'])
plt.subplot(222)
df['FLAG_OWN_CAR'].value_counts(normalize=True).plot.bar(title= 'Does The Client Own a Car?', color=['red','blue'])
plt.subplot(223)
df['CNT_CHILDREN'].value_counts(normalize=True).plot.bar(title= 'How Many Children does the Client have?', color=['red','blue','blue','blue'])
plt.subplot(224)
df['FLAG_OWN_REALTY'].value_counts(normalize=True).plot.bar(figsize=(8,8), title= 'Does The Client Own Realty?', color=['red','blue'])

plt.show()

df['CODE_GENDER'].value_counts()

df['CNT_CHILDREN'].value_counts()

df['FLAG_OWN_CAR'].value_counts()

df['FLAG_OWN_REALTY'].value_counts()

df['EXT_SOURCE_1'].describe()

data_EX1 = df[['TARGET', 'EXT_SOURCE_1']]
data_EX1['EXT_SOURCE_1_GROUP'] = pd.cut(data_EX1['EXT_SOURCE_1'], bins = np.linspace(0, 1, num=6))
data_EX1 = (data_EX1.groupby(['EXT_SOURCE_1_GROUP']).mean()).sort_values('TARGET')
data_EX1

plt.barh(data_EX1.index.astype(str), round(100*data_EX1['TARGET']), color='darkblue')

df['EXT_SOURCE_2'].describe()

data_EX2 = df[['TARGET', 'EXT_SOURCE_2']]
data_EX2['EXT_SOURCE_2_GROUP'] = pd.cut(data_EX2['EXT_SOURCE_2'], bins = np.linspace(0, 1, num=6))
data_EX2 = (data_EX2.groupby(['EXT_SOURCE_2_GROUP']).mean()).sort_values('TARGET')
data_EX2

plt.barh(data_EX2.index.astype(str), round(100*data_EX2['TARGET']), color='grey')

df['EXT_SOURCE_3'].describe()

data_EX3 = df[['TARGET', 'EXT_SOURCE_3']]
data_EX3['EXT_SOURCE_3_GROUP'] = pd.cut(data_EX3['EXT_SOURCE_3'], bins = np.linspace(0, 1, num=6))
data_EX3 = (data_EX3.groupby(['EXT_SOURCE_3_GROUP']).mean()).sort_values('TARGET')
data_EX3

plt.barh(data_EX3.index.astype(str), round(100*data_EX3['TARGET']), color='darkblue')

"""## Data Prepocessing Data Training"""

df = (apptrain.merge(prev_app, how='left', on='SK_ID_CURR')).merge(bureau_cred, how='left', on='SK_ID_CURR')
df

df['DAYS_BIRTH']

df['AGE'] = df['DAYS_BIRTH']/-365
df.drop(columns='DAYS_BIRTH', inplace=True)

df['AGE'].describe()

df['DAYS_EMPLOYED']

df['DAYS_EMPLOYED'].describe()

df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED']/-365
df.drop(columns='DAYS_EMPLOYED', inplace=True)

df['YEARS_EMPLOYED'].describe()

df['YEARS_EMPLOYED'].replace({df['YEARS_EMPLOYED'].min(): np.nan}, inplace=True)
df['YEARS_EMPLOYED'].plot.hist(title = 'Years Employment Histogram');
plt.xlabel('Years Employment');

data_employed = df[['TARGET', 'YEARS_EMPLOYED','AGE']]
data_employed['YEARS_EMPLOYED_GROUP'] = pd.cut(data_employed['YEARS_EMPLOYED'], bins = np.linspace(0, 50, num=6))
data_employed = ((data_employed.groupby(['YEARS_EMPLOYED_GROUP']).mean())).sort_values('TARGET')
data_employed

plt.barh(data_employed.index.astype(str), round(100*data_employed['TARGET']), color='darkblue')

plt.ylabel('Years Employed Group (years)')
plt.xlabel('Failure to Repay (%)')
plt.title('Failure to Repay by Years Employed Group');
plt.show()

plt.barh(data_employed.index.astype(str), round(data_employed['AGE']), color='darkblue')

plt.ylabel('Years Employed Group (years)')
plt.xlabel('Average Age')
plt.title('Average Age vs Years Employed Group');
plt.show()

"""**Number of documents provided by a customer**"""

import re

regex = re.compile('FLAG_DOCUMENT_')
col = [i for i in df.columns if re.match(regex, i)]

df['TOTAL_DOCUMENT'] = df['FLAG_DOCUMENT_2'] + df['FLAG_DOCUMENT_3'] + df['FLAG_DOCUMENT_4'] + df['FLAG_DOCUMENT_5'] + df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_7'] + df['FLAG_DOCUMENT_8'] + df['FLAG_DOCUMENT_9'] + df['FLAG_DOCUMENT_10'] + df['FLAG_DOCUMENT_11'] + df['FLAG_DOCUMENT_12'] + df['FLAG_DOCUMENT_13'] + df['FLAG_DOCUMENT_14'] + df['FLAG_DOCUMENT_15'] + df['FLAG_DOCUMENT_16'] + df['FLAG_DOCUMENT_17'] + df['FLAG_DOCUMENT_18'] + df['FLAG_DOCUMENT_19'] + df['FLAG_DOCUMENT_20'] + df['FLAG_DOCUMENT_21']

df.drop(columns = [i for i in df.columns if re.match(regex, i)], inplace=True)

df['TOTAL_DOCUMENT'].value_counts()

doc_list = []
for index, column in df.iterrows():
    if column['TOTAL_DOCUMENT'] >= 1:
        doc = 1
    else:
        doc = 0
    doc_list.append(doc)

df['DOC_PROVIDED'] = doc_list

doc_data = df.groupby(['DOC_PROVIDED'])['TARGET'].mean().reset_index()
doc_data

doc_data['DOC_PROVIDED'] = doc_data['DOC_PROVIDED'].astype('int')

plt.barh(range(0,2), round(100*doc_data['TARGET']), color='darkblue')

plt.ylabel('Client Provide Document or Not')
plt.xlabel('Failure to Repay (%)')
plt.title('Failure to Repay by Provided Document')
plt.show()

"""**Income Annuity Percentage** : persentase anuitas relatif terhadap pendapatan klien

Anuitas pendapatan adalah jaminan pendapatan seumur hidup yang Anda beli dari perusahaan asuransi untuk mengurangi risiko kehabisan uang di masa pensiun. Sama seperti Anda mengasuransikan rumah Anda, Anda dapat memastikan umur panjang Anda dengan meneruskan risiko pengeluaran tabungan Anda ke perusahaan asuransi.

Source: https://www.blueprintincome.com/resources/income-annuities/
"""

df['INCOME_ANNUITY_PERCENT'] = (df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'])*100

df['INCOME_ANNUITY_PERCENT'].describe()

income_list = []
for index, column in df.iterrows():
    if column['INCOME_ANNUITY_PERCENT'] > 150:
        income = 150
    else:
        income = column['INCOME_ANNUITY_PERCENT']
    income_list.append(income)

df['INCOME_ANNUITY_PERCENT_NEW'] = income_list

income_annuity = df[['TARGET', 'INCOME_ANNUITY_PERCENT_NEW']]
income_annuity['INCOME_ANNUITY_PERCENT_NEW_GROUP'] = pd.cut(income_annuity['INCOME_ANNUITY_PERCENT_NEW'], bins = np.linspace(0, 200, num=5))
income_annuity = (income_annuity.groupby(['INCOME_ANNUITY_PERCENT_NEW_GROUP']).mean()).sort_values('TARGET')
income_annuity

"""**Earned Income Tax Credit (EITC)**

EITC sama dengan persentase tetap ("tingkat kredit") dari pendapatan yang diperoleh sampai kredit mencapai jumlah maksimumnya.

Source: https://sgp.fas.org/crs/misc/R43805.pdf
"""

df['EITC'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']

plt.figure(figsize = (5,5))
# iterate through the new features
for i, feature in enumerate(['INCOME_ANNUITY_PERCENT', 'EITC']):
    
    # create a new subplot for each source
    plt.subplot(2, 1, i + 1)
    # plot repaid loans
    sns.kdeplot(df.loc[df['TARGET'] == 0, feature], label = 'target == 0')
    # plot loans that were not repaid
    sns.kdeplot(df.loc[df['TARGET'] == 1, feature], label = 'target == 1')
    
    # Label the plots
    plt.title('Distribution of %s by Target Value' % feature)
    plt.xlabel('%s' % feature); plt.ylabel('Density');
    
plt.tight_layout(h_pad = 2.5)

pd.set_option('display.max_columns', None)
df

"""***Ganti nilai XNA dengan NaN***"""

for col in df.select_dtypes(include = ["object"]).columns:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

df.CODE_GENDER.replace("XNA", np.nan, inplace=True)

"""#### Menjaga kolom yang kurang dari sama dengan 60% dari nilai yang hilang

Melihat di himpunan data ketika ada lebih dari 60% data hilang, biasanya variabel drop yang paling disukai ketika melibatkan dan mengambil pilihan untuk menjatuhkan variabel yang seharusnya tidak memengaruhi analisis keseluruhan.

Source: https://www.analyticsvidhya.com/blog/2021/10/guide-to-deal-with-missing-values/
"""

df.shape

total = df.isnull().sum().sort_values(ascending = False)
percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)
missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).reset_index()
missing_data.loc[missing_data['Percent'] >60]

df.drop(columns=list(missing_data['index'].loc[missing_data['Percent'] >60]),inplace=True)
df.shape

"""### Isi missing value dengan imputation"""

null = df.isnull().sum().reset_index()
null_table = null.loc[null[0] > 0]
null_table.shape

"""ada 56 kolom yang memiliki missing value"""

data_null = df[null_table['index'].tolist()]
data_null

data_null.select_dtypes(exclude = ["object"] ).shape[1]

data_null.select_dtypes(include = ["object"] ).shape[1]

def miss_numerical(df):
    
    numerical_features = df.select_dtypes(exclude = ["object"] ).columns
    for f in numerical_features:
        df[f] = df[f].fillna(df[f].median())
    return df

def miss_categorical(df):
    
    categorical_features = df.select_dtypes(include = ["object"]).columns
    for f in categorical_features:
        df[f] = df[f].fillna(df[f].mode()[0])
    return df

def transform_feature(df):
    df = miss_numerical(df)
    df = miss_categorical(df)
    return df

df = transform_feature(df)
df

"""kita cek lagi apakah masih terdapat missing value atau tidak"""

null = df.isnull().sum().reset_index()
null_table = null.loc[null[0] > 0]
null_table.shape[0]

"""tidak ada missing value

### Scaling Numerical Features
"""

from sklearn.preprocessing import MinMaxScaler


def encoder(df):
    scaler = MinMaxScaler()
    numerical = df.select_dtypes(exclude = ["object"]).columns
    numerical1 = numerical[2:4]
    numerical2 = numerical[5:]
    numerical_all = numerical1 | numerical2
    features_transform = pd.DataFrame(data=df)
    features_transform[numerical_all] = scaler.fit_transform(df[numerical_all])
    display(features_transform.head(n = 5))
    return df

df = encoder(df)

scaler_AMT_CREDIT = MinMaxScaler()
scaler_AMT_CREDIT.fit(df['AMT_CREDIT'].values.reshape(len(df), 1))

df['AMT_CREDIT'] = scaler_AMT_CREDIT.transform(df['AMT_CREDIT'].values.reshape(len(df), 1))

df.shape

"""### Feature Encoding

kategori yg memiliki 2 kategori unik menggunakan label encoding, sedangkan untuk yang lebih dari 2 kategori unik menggunakan one-hot encoding
"""

df[df.select_dtypes(include = ["object"]).columns].head()

for col in df.select_dtypes(include = ["object"]).columns:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le_count = 0

for col in df:
    if df[col].dtype == 'object':
        if len(list(df[col].unique())) <= 2:
            le.fit(df[col])
            df[col] = le.transform(df[col])            
            le_count += 1
           
print('Total label encoded columns: %d' % le_count)

df.shape

le_count = 0

for col in df:
    if df[col].dtype == 'object':
        if len(list(df[col].unique())) > 2:
            onehots = pd.get_dummies(df[col])
            df = df.join(onehots)
            df.drop(columns=col, inplace=True)
            le_count += 1
           
print('Total one hot encoded columns: %d' % le_count)

df.shape

df.info()

df_train = df.iloc[:,1:]

DF_TRAIN = df_train
DF_TRAIN

"""## Data Processing Data Test"""

df = (apptes.merge(prev_app, how='left', on='SK_ID_CURR')).merge(bureau_cred, how='left', on='SK_ID_CURR')
df

"""Konvert DAYS_BIRTH menjadi AGE utk mendapatkan umur klien"""

df['AGE'] = df['DAYS_BIRTH']/-365
df.drop(columns='DAYS_BIRTH', inplace=True)

df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED']/-365
df.drop(columns='DAYS_EMPLOYED', inplace=True)

df['YEARS_EMPLOYED'].replace({df['YEARS_EMPLOYED'].min(): np.nan}, inplace=True)

"""#### Number of documents provided by a customer"""

regex = re.compile('FLAG_DOCUMENT_')
col = [i for i in df.columns if re.match(regex, i)]

df['TOTAL_DOCUMENT'] = df['FLAG_DOCUMENT_2'] + df['FLAG_DOCUMENT_3'] + df['FLAG_DOCUMENT_4'] + df['FLAG_DOCUMENT_5'] + df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_7'] + df['FLAG_DOCUMENT_8'] + df['FLAG_DOCUMENT_9'] + df['FLAG_DOCUMENT_10'] + df['FLAG_DOCUMENT_11'] + df['FLAG_DOCUMENT_12'] + df['FLAG_DOCUMENT_13'] + df['FLAG_DOCUMENT_14'] + df['FLAG_DOCUMENT_15'] + df['FLAG_DOCUMENT_16'] + df['FLAG_DOCUMENT_17'] + df['FLAG_DOCUMENT_18'] + df['FLAG_DOCUMENT_19'] + df['FLAG_DOCUMENT_20'] + df['FLAG_DOCUMENT_21']

df.drop(columns = [i for i in df.columns if re.match(regex, i)], inplace=True)

"""### Income Annuity Percentage
persentase anuitas relatif terhadap pendapatan klien

Anuitas pendapatan adalah jaminan pendapatan seumur hidup yang Anda beli dari perusahaan asuransi untuk mengurangi risiko kehabisan uang di masa pensiun. Sama seperti Anda mengasuransikan rumah Anda, Anda dapat memastikan umur panjang Anda dengan meneruskan risiko pengeluaran tabungan Anda ke perusahaan asuransi.

Source: https://www.blueprintincome.com/resources/income-annuities/
"""

df['INCOME_ANNUITY_PERCENT'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']

df['INCOME_ANNUITY_PERCENT'].describe()

"""### Earned Income Tax Credit (EITC)

EITC sama dengan persentase tetap ("tingkat kredit") dari pendapatan yang diperoleh sampai kredit mencapai jumlah maksimumnya.

Source: https://sgp.fas.org/crs/misc/R43805.pdf
"""

df['EITC'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']
df

for col in df.select_dtypes(include = ["object"]).columns:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

df.ORGANIZATION_TYPE.replace("XNA", np.nan, inplace=True)

"""#### Menjaga kolom yang kurang dari sama dengan 60% dari nilai yang hilang

Melihat di himpunan data ketika ada lebih dari 60% data hilang, biasanya variabel drop yang paling disukai ketika melibatkan dan mengambil pilihan untuk menjatuhkan variabel yang seharusnya tidak memengaruhi analisis keseluruhan.

Source: https://www.analyticsvidhya.com/blog/2021/10/guide-to-deal-with-missing-values/
"""

df.shape

total = df.isnull().sum().sort_values(ascending = False)
percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)
missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).reset_index()
missing_data.loc[missing_data['Percent'] >60]

df.drop(columns=list(missing_data['index'].loc[missing_data['Percent'] >60]),inplace=True)
df.shape

df.head()

"""#### isi missing values dengan imputasi"""

null = df.isnull().sum().reset_index()
null_table = null.loc[null[0] > 0]
null_table.shape

"""terdapat 53 kolom memiliki missing value"""

data_null = df[null_table['index'].tolist()]
data_null

data_null.select_dtypes(exclude = ["object"] ).shape[1]

data_null.select_dtypes(include = ["object"] ).shape[1]

def miss_numerical(df):
    
    numerical_features = df.select_dtypes(exclude = ["object"] ).columns
    for f in numerical_features:
        df[f] = df[f].fillna(df[f].median())
    return df

def miss_categorical(df):
    
    categorical_features = df.select_dtypes(include = ["object"]).columns
    for f in categorical_features:
        df[f] = df[f].fillna(df[f].mode()[0])
    return df

def transform_feature(df):
    df = miss_numerical(df)
    df = miss_categorical(df)
    return df

df = transform_feature(df)
df

"""cek missing value lagi"""

null = df.isnull().sum().reset_index()
null_table = null.loc[null[0] > 0]
null_table.shape[0]

"""tidak ada missing value

### Scaling numerical features
"""

def encoder(df):
    scaler = MinMaxScaler()
    numerical = df.select_dtypes(exclude = ["object"]).columns
    numerical = numerical[1:]
    features_transform = pd.DataFrame(data=df)
    features_transform[numerical] = scaler.fit_transform(df[numerical])
    display(features_transform.head(n = 5))
    return df

df = encoder(df)

df.shape

"""### Feature Encoding

kategori yg memiliki 2 kategori unik menggunakan label encoding, sedangkan untuk yang lebih dari 2 kategori unik menggunakan one-hot encoding
"""

df[df.select_dtypes(include = ["object"]).columns].head()

for col in df.select_dtypes(include = ["object"]).columns:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

le = LabelEncoder()
le_count = 0

for col in df:
    if df[col].dtype == 'object':
        if len(list(df[col].unique())) <= 2:
            le.fit(df[col])
            df[col] = le.transform(df[col])            
            le_count += 1
           
print('%d columns were label encoded.' % le_count)

df.shape

le_count = 0

for col in df:
    if df[col].dtype == 'object':
        if len(list(df[col].unique())) > 2:
            onehots = pd.get_dummies(df[col])
            df = df.join(onehots)
            df.drop(columns=col, inplace=True)
            le_count += 1
           
print('%d columns were one hot encoded.' % le_count)

df.shape

df.head()

df.info()

df_test = df.iloc[:,1:]
df_test

DF_TEST = df_test
DF_TEST

"""## Menyelaraskan Data Test dan Data Train
One-hot encoding telah membuat lebih banyak kolom dalam data pelatihan karena ada beberapa variabel kategoris dengan kategori yang tidak terwakili dalam data pengujian. Untuk menghapus kolom dalam data pelatihan yang tidak ada dalam data pengujian, kita perlu menyelaraskan dataframes. 
"""

print('Training Features shape before: ', df_train.shape)
print('Testing Features shape before: ', df_test.shape)

target = df_train['TARGET']

df_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)
df_train['TARGET'] = target

print('Training Features shape after: ', df_train.shape)
print('Testing Features shape after: ', df_test.shape)

"""## WOE and IV"""

df_train['TARGET'].value_counts()

plt.figure(figsize=(8,6),dpi=100)
plt.title("Proportion Good and Bad Borrower",fontsize=12)
plt.pie(df_train['TARGET'].value_counts(),labels=["Good","Bad"],
       autopct='%1.1f%%',colors=["b","y"]);

"""**WOE** atau Weight of Evidence kita gunakan untuk mengukur antara nasabah baik dan buruk
nilai dari **WOE** ini akan digunakan untuk menghitung **IV** atau Infromation Value
"""

import pandas as pd
import scipy.stats as stats

#categoric fitur dan continuous fiture

class CategoricalFeature():
    def __init__(self, df_train, feature):
        self.df = df_train
        self.feature = feature

    @property
    def df_lite(self):
        df_lite = self.df
        df_lite['bin'] = df_lite[self.feature].fillna('MISSING')
        return df_lite[['bin', 'TARGET']]


class ContinuousFeature():
    def __init__(self, df_train, feature):
        self.df = df_train
        self.feature = feature
        self.bin_min_size = int(len(self.df) * 0.05)

    def __generate_bins(self, bins_num):
        df_train = self.df[[self.feature, 'TARGET']]
        df_train['bin'] = pd.qcut(df_train[self.feature], bins_num, duplicates='drop') \
                    .apply(lambda x: x.left) \
                    .astype(float)
        return df_train

    def __generate_correct_bins(self, bins_max=20):
        for bins_num in range(bins_max, 1, -1):
            df_train = self.__generate_bins(bins_num)
            df_grouped = pd.DataFrame(df_train.groupby('bin') \
                                      .agg({self.feature: 'count',
                                            'TARGET': 'sum'})) \
                                      .reset_index()
            r, p = stats.stats.spearmanr(df_grouped['bin'], df_grouped['TARGET'])

            if (
                    abs(r)==1 and                                                        # periksa apakah WOE untuk tiap bin adalah monotonic
                    df_grouped[self.feature].min() > self.bin_min_size                   # periksa apakah size setiap bin besar dari 5%
                    and not (df_grouped[self.feature] == df_grouped['TARGET']).any()      # periksa apakah label Yes dan No jumlahnya adalah 0
            ):
                break

        return df_train

    @property
    def df_lite(self):
        df_lite = self.__generate_correct_bins()
        df_lite['bin'].fillna('MISSING', inplace=True)
        return df_lite[['bin', 'TARGET']]

"""Information Value kita gunakan untuk menghitung seberapa penting dan prediktif terhadap target"""

# Information Value
pd.set_option('mode.chained_assignment', None)

class AttributeRelevance():
    def seq_palette(self, n_colors):
        return sns.cubehelix_palette(n_colors, start=.5, rot=-.75, reverse=True)

    def bulk_iv(self, feats, iv, woe_extremes=False):
        iv_dict = {}
        for f in feats:
            iv_df, iv_value = iv.calculate_iv(f)
            if woe_extremes:
                iv_dict[f.feature] = [iv_value, iv_df['woe'].min(), iv_df['woe'].max()]
                cols = ['iv', 'woe_min', 'woe_max']
            else:
                iv_dict[f.feature] = iv_value
                cols = ['iv']
        df_train = pd.DataFrame.from_dict(iv_dict, orient='index', columns=cols)
        return df_train

    def bulk_stats(self, feats, s):
        stats_dict = {}
        for f in feats:
            p_value, effect_size = s.calculate_chi(f)
            stats_dict[f.feature] = [p_value, effect_size]
        df_train = pd.DataFrame.from_dict(stats_dict, orient='index', columns=['p-value', 'effect_size'])
        return df_train

    def analyze(self, feats, iv, s=None, interpretation=False):
        df_iv = self.bulk_iv(feats, iv).sort_values(by='iv', ascending=False)
        if s is not None:
            df_stats = self.bulk_stats(feats, s)
            df_iv = df_iv.merge(df_stats, left_index=True, right_index=True)
        if interpretation:
            df_iv['iv_interpretation'] = df_iv['iv'].apply(iv.interpretation)
            if s is not None:
                df_iv['es_interpretation'] = df_iv['effect_size'].apply(s.interpretation)
        return df_iv

    def draw_iv(self, feats, iv):
        df_train = self.analyze(feats, iv)
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x=df_train.index, y='iv', data=df_train, palette=self.seq_palette(len(feats)))
        ax.set_title('IV values')
        plt.xticks(rotation=90)
        plt.show()

    def draw_woe_extremes(self, feats, iv):
        df_train = self.bulk_iv(feats, iv, woe_extremes=True).sort_values(by='iv', ascending=False)
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x=df_train.index, y='woe_min', data=df_train, palette=self.seq_palette(len(feats)))
        sns.barplot(x=df_train.index, y='woe_max', data=df_train, palette=self.seq_palette(len(feats)))
        ax.axhline(y=0, color='black', linewidth=1)
        ax.set_title('Range of WOE values')
        ax.set_ylabel('WOE')
        plt.xticks(rotation=90)
        plt.show()

    def draw_woe_multiplot(self, feats, iv):
        n = len(feats)
        nrows = int(np.ceil(n/3))
        fig, ax = plt.subplots(nrows=nrows, ncols=3, figsize=(15, nrows*4))
        for i in range(n):
            iv_df, iv_value = iv.calculate_iv(feats[i])
            sns.barplot(x=feats[i].feature, y='woe', data=iv_df, color='#455872', ax=fig.axes[i])

        for ax in fig.axes:
            plt.sca(ax)
            plt.xticks(rotation=50)

        plt.tight_layout()
        plt.show()

class Analysis():
    def seq_palette(self, n_colors):
        return sns.cubehelix_palette(n_colors, start=.5, rot=-.75, reverse=True)

    def group_by_feature(self, feat):
        df_train = feat.df_lite \
                            .groupby('bin') \
                            .agg({'TARGET': ['count', 'sum']}) \
                            .reset_index()
        df_train.columns = [feat.feature, 'count', 'good']
        df_train['bad'] = df_train['count'] - df_train['good']
        return df_train

class StatsSignificance(Analysis):
    def calculate_chi(self, feat):
        df_train = self.group_by_feature(feat)
        df_chi = np.array(df_train[['good', 'bad']])
        n = df_train['count'].sum()

        chi = stats.chi2_contingency(df_chi)
        cramers_v = np.sqrt(chi[0] / n)          # assume that k=2 (good, bad)
        return chi[1], cramers_v

    @staticmethod
    def interpretation(cramers_v):
        if cramers_v < 0.1:
            return 'useless'
        elif cramers_v < 0.2:
            return 'weak'
        elif cramers_v < 0.4:
            return 'medium'
        elif cramers_v < 0.6:
            return 'strong'
        else:
            return 'very strong'

    def interpret_chi(self, feat):
        _, cramers_v = self.calculate_chi(feat)
        return self.interpretation(cramers_v)

    def print_chi(self, feat):
        p_value, cramers_v = self.calculate_chi(feat)
        print('P-value: %0.2f\nEffect size: %0.2f' % (p_value, cramers_v))
        print('%s is a %s predictor' % (feat.feature.capitalize(), self.interpretation(cramers_v)))


class IV(Analysis):
    @staticmethod
    def __perc_share(df_train, group_name):
        return df_train[group_name] / df_train[group_name].sum()

    def __calculate_perc_share(self, feat):
        df_train = self.group_by_feature(feat)
        df_train['perc_good'] = self.__perc_share(df_train, 'good')
        df_train['perc_bad'] = self.__perc_share(df_train, 'bad')
        df_train['perc_diff'] = df_train['perc_good'] - df_train['perc_bad']
        return df_train

    def __calculate_woe(self, feat):
        df_train = self.__calculate_perc_share(feat)
        df_train['woe'] = np.log(df_train['perc_good']/df_train['perc_bad'])
        df_train['woe'] = df_train['woe'].replace([np.inf, -np.inf], np.nan).fillna(0)
        return df_train

    def calculate_iv(self, feat):
        df_train = self.__calculate_woe(feat)
        df_train['iv'] = df_train['perc_diff'] * df_train['woe']
        return df_train, df_train['iv'].sum()

    def draw_woe(self, feat):
        iv_df, iv_value = self.calculate_iv(feat)
        fig, ax = plt.subplots(figsize=(10,6))
        sns.barplot(x=feat.feature, y='woe', data=iv_df, palette=self.seq_palette(len(iv_df.index)))
        ax.set_title('WOE visualization for: ' + feat.feature)
        plt.show()
        plt.show()

    @staticmethod
    def interpretation(iv):
        if iv < 0.02:
            return 'useless'
        elif iv < 0.1:
            return 'weak'
        elif iv < 0.3:
            return 'medium'
        elif iv < 0.5:
            return 'strong'
        else:
            return 'suspicious'

    def interpret_iv(self, feat):
        _, iv = self.calculate_iv(feat)
        return self.interpretation(iv)

    def print_iv(self, feat):
        _, iv = self.calculate_iv(feat)
        print('Information value: %0.2f' % iv)
        print('%s is a %s predictor' % (feat.feature.capitalize(), self.interpretation(iv)))

from pandas.core.dtypes.common import is_numeric_dtype
#selection feat
feats_dict = {}

for col in [c for c in df_train.columns if c != 'TARGET']:
  if is_numeric_dtype(df_train[col]):
    feats_dict[col] = ContinuousFeature(df_train,col)
  else :
    feats_dict[col] = CategoricalFeature(df_train,col)

feats = list(feats_dict.values())

iv = IV()
s = StatsSignificance()

ar = AttributeRelevance()


df_analysis = ar.analyze(feats, iv, s, interpretation=True)
display(df_analysis)

df_analysis_sign = df_analysis[df_analysis['p-value']<0.05]

fig, ax = plt.subplots(figsize=(10,6))
sns.regplot(x='iv', y='effect_size', data=df_analysis_sign, color='#455872')
ax.set_title('Information value vs effect size')
plt.show()

print('Pearson correlation: %0.2f' % df_analysis_sign['iv'].corr(df_analysis_sign['effect_size']))
print('Spearman correlation: %0.2f' % df_analysis_sign['iv'].corr(df_analysis_sign['effect_size'], method='spearman'))

ar.draw_iv(feats, iv)

ar.draw_woe_multiplot(feats,iv)

#select fitur
feature_keep = df_analysis[df_analysis["iv"] > 0.01].index
feature_keep

feature_keep = ['EXT_SOURCE_2', 'EXT_SOURCE_3', 'EXT_SOURCE_1', 'AGE', 'YEARS_EMPLOYED',
       'DAYS_LAST_PHONE_CHANGE', 'DAYS_ID_PUBLISH', 'AMT_GOODS_PRICE',
       'REGION_RATING_CLIENT_W_CITY', 'FLOORSMAX_AVG', 'REGION_RATING_CLIENT',
       'FLOORSMAX_MEDI', 'FLOORSMAX_MODE', 'LIVINGAREA_MEDI', 'LIVINGAREA_AVG',
       'TOTALAREA_MODE', 'LIVINGAREA_MODE', 'DAYS_REGISTRATION',
       'ELEVATORS_AVG', 'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'APARTMENTS_AVG',
       'ELEVATORS_MODE', 'APARTMENTS_MODE', 'YEARS_BEGINEXPLUATATION_MEDI',
       'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BEGINEXPLUATATION_MODE',
       'BASEMENTAREA_AVG', 'ENTRANCES_AVG', 'BASEMENTAREA_MEDI',
       'ENTRANCES_MEDI', 'ENTRANCES_MODE', 'NONLIVINGAREA_MODE',
       'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI', 'BASEMENTAREA_MODE',
       'LANDAREA_AVG', 'LANDAREA_MEDI', 'LANDAREA_MODE','TARGET']

#for splitting
df_train = df_train[feature_keep]

#check correlation
fig = plt.figure(figsize = (20,20))
matrix = np.triu(df_train.corr())
sns.heatmap(df_train.corr(), center = 0,
           fmt='.3f', square = True,
           annot = True, linewidth = 0.3, mask = matrix)
plt.show()

#make a binning data 
target = "TARGET"
bins = sc.woebin(df_train,target,monotonic_trend="auto_asc_desc")

sc.woebin_plot(bins)

def woebin_plot_new(bins, x=None, title=None, show_iv=True):
    
    xs = x
    # bins concat 
    if isinstance(bins, dict):
        bins = pd.concat(bins, ignore_index=True)
    # good bad distr
    def gb_distr(binx):
        binx['good_distr'] = binx['good']/sum(binx['count'])
        binx['bad_distr'] = binx['bad']/sum(binx['count'])
        return binx
    bins = bins.groupby('variable').apply(gb_distr)
    # x variable names
    if xs is None: xs = bins['variable'].unique()
    # plot export
    plotlist = {}
    for i in xs:
        binx = bins[bins['variable'] == i].reset_index()
        plotlist[i] = plot_bin_new(binx, title, show_iv)
    return plotlist

def plot_bin_new(binx, title, show_iv):
  
    # y_right_max
    y_right_max = np.ceil(binx['badprob'].max()*10)
    if y_right_max % 2 == 1: y_right_max=y_right_max+1
    if y_right_max - binx['badprob'].max()*10 <= 0.3: y_right_max = y_right_max+2
    y_right_max = y_right_max/10
    if y_right_max>1 or y_right_max<=0 or y_right_max is np.nan or y_right_max is None: y_right_max=1
    ## y_left_max
    y_left_max = np.ceil(binx['count_distr'].max()*10)/10
    if y_left_max>1 or y_left_max<=0 or y_left_max is np.nan or y_left_max is None: y_left_max=1
    # title
    title_string = binx.loc[0,'variable']+"  (iv:"+str(round(binx.loc[0,'total_iv'],4))+")" if show_iv else binx.loc[0,'variable']
    title_string = title+'-'+title_string if title is not None else title_string
    # param
    ind = np.arange(len(binx.index))    # the x locations for the groups
    width = 0.5       # the width of the bars: can also be len(x) sequence
    ###### plot ###### 
    fig, ax1 = plt.subplots(figsize=(12,6))
    ax2 = ax1.twinx()
    # ax1
    p1 = ax1.bar(ind, binx['good_distr'], width, color=(24/254, 192/254, 196/254))
    p2 = ax1.bar(ind, binx['bad_distr'], width, bottom=binx['good_distr'], color=(246/254, 115/254, 109/254))
    for i in ind:
        ax1.text(i, binx.loc[i,'count_distr']*1.02, str(round(binx.loc[i,'count_distr']*100,1))+'%, '+str(binx.loc[i,'count']), ha='center')
    # ax2
    ax2.plot(ind, binx['badprob'], marker='o', color='blue')
    for i in ind:
        ax2.text(i, binx.loc[i,'badprob']*1.02, str(round(binx.loc[i,'badprob'], 2)) , color='blue', ha='center')
    # settings
    ax1.set_ylabel('Bin count distribution')
    ax2.set_ylabel('Bad Probability', color='blue')
    ax1.set_yticks(np.arange(0, y_left_max+0.2, 0.2))
    ax2.set_yticks(np.arange(0, y_right_max+0.2, 0.2))
    ax2.tick_params(axis='y', colors='blue')
    ax1.tick_params(axis='x', rotation=45)
    plt.xticks(ind, binx['bin'])
#     plt.xticks(rotation = 45)
#     plt.figure(figsize=(12,6))
    plt.rcParams['font.size'] = '14'
    plt.title(title_string, loc='left')
    plt.legend((p2[0], p1[0]), ('bad', 'good'), loc='upper right')
    # show plot 
    # plt.show()
    return fig

woebin_plot_new(bins)

#transsform to woe
df_train = sc.woebin_ply(df_train, bins)

fig = plt.figure(figsize = (20,20))
matrix = np.triu(df.corr())
sns.heatmap(df.corr(), center = 0,
           fmt='.3f', square = True,
           annot = True, linewidth = 0.3, mask = matrix)
plt.show()

"""## Model Building

### Split Data
"""

X = df_train.drop(columns = ['TARGET'])
Y = df_train[['TARGET']]
X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""### Imbalance Dataset"""

y_train['TARGET'].value_counts()

x = X_train[[col for col in X_train.columns if (str(X_train[col].dtype) != 'object')]]
y = y_train['TARGET'].values
print(x.shape)
print(y.shape)

from imblearn.over_sampling import SMOTE
x_over_SMOTE, y_over_SMOTE = SMOTE(0.5).fit_resample(x, y)
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())

data_train = x_over_SMOTE
data_train['TARGET'] = pd.DataFrame(y_over_SMOTE)

print('Original')
print(pd.Series(y).value_counts())
print('')
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())

X_train = data_train.drop(columns = ['TARGET'])
y_train = data_train[['TARGET']]

def eval_classification(model, pred, xtrain, ytrain, xtest, ytest):
    print("Accuracy (Test Set): %.2f" % accuracy_score(ytest, pred))
    print("Precision (Test Set): %.2f" % precision_score(ytest, pred))
    print("Recall (Test Set): %.2f" % recall_score(ytest, pred))
    print("F1-Score (Test Set): %.2f" % f1_score(ytest, pred))
    
    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive
    print("AUC: %.2f" % auc(fpr, tpr))

"""### Logistic Regression"""

logreg = LogisticRegression(random_state=42)
logreg.fit(X_train,y_train)

preds = logreg.predict(X_train)

accuracy = accuracy_score(preds,y_train)
print(accuracy)

preds_test = logreg.predict(X_test)

accuracy = accuracy_score(preds_test,y_test)
print(accuracy)

false_positive_rate, true_positive_rate, threshold = roc_curve(y_train,logreg.predict_proba(X_train)[:,1])
roc_auc_value = roc_auc_score(y_train, logreg.predict_proba(X_train)[:,1]).round(4)
gini_value = ((2*roc_auc_value)-1).round(4)

print('AUC for Logistic Regression on val data: ', round(roc_auc_value*100, 2), '%')
print('Gini for Logistic Regression on val data: ', round(gini_value*100, 2), '%')

false_positive_rate, true_positive_rate, threshold = roc_curve(y_test,logreg.predict_proba(X_test)[:,1])
roc_auc_value = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]).round(4)
gini_value = ((2*roc_auc_value)-1).round(4)

print('AUC for Logistic Regression on val data: ', round(roc_auc_value*100, 2), '%')
print('Gini for Logistic Regression on val data: ', round(gini_value*100, 2), '%')

y_train_pred = logreg.predict(X_train)
cnf_matrix = metrics.confusion_matrix(y_train, y_train_pred)
print(metrics.classification_report(y_train, y_train_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

y_test_pred = logreg.predict(X_test)
cnf_matrix = metrics.confusion_matrix(y_test, y_test_pred)
print(metrics.classification_report(y_test,y_test_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

feat_importances = pd.Series(logreg.coef_[0], index=X.columns)
ax = feat_importances.nlargest(32).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('Score')
plt.ylabel('Feature')
plt.title('Feature Importance Score')
plt.show()

feat_importances.abs().sort_values(ascending = False)

#recursive Elimination
from sklearn.feature_selection import RFE
model = LogisticRegression(solver='lbfgs', max_iter=500)
rfe = RFE(model)
rfe = rfe.fit(X_train, y_train)

X_train.columns[rfe.support_]

X_train_new = X_train[X_train.columns[rfe.support_]]
X_test_new = X_test[X_test.columns[rfe.support_]]

fig = plt.figure(figsize = (20,20))
matrix = np.triu(X_train_new.corr())
sns.heatmap(X_train_new.corr(), center = 0,
           fmt='.3f', square = True,
           annot = True, linewidth = 0.3, mask = matrix)
plt.show()

param_grid = {"penalty": ["l1","l2"],
             'C' : [1.0,2.0,3.0],
             'max_iter':[100,200,300,500],
             'solver' : ['newton-cg','lbfgs','sag','saga','liblinear'],
             }

model = LogisticRegression()#class_weight="balanced")
grid_search = RandomizedSearchCV(model,param_grid,cv=5)
grid_search.fit(X_train_new,y_train)

grid_search.best_params_

y_train_pred = grid_search.best_estimator_.predict(X_train_new)
y_test_pred = grid_search.best_estimator_.predict(X_test_new)

false_positive_rate, true_positive_rate, threshold = roc_curve(y_train,grid_search.predict_proba(X_train_new)[:,1])
roc_auc_value = roc_auc_score(y_train, grid_search.predict_proba(X_train_new)[:,1]).round(4)
gini_value = ((2*roc_auc_value)-1).round(4)

print('AUC for Logistic Regression on val data: ', round(roc_auc_value*100, 2), '%')
print('Gini for Logistic Regression on val data: ', round(gini_value*100, 2), '%')

fig, ax = plt.subplots(1, figsize=(8,6))
plt.title('Receiver Operating Characteristic - LogReg Classifier training')
plt.plot(false_positive_rate, true_positive_rate)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".7"), plt.plot([1, 1] , c=".7")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

plt.text(ax.get_xlim()[1]*6/10, 
         0, 
         f"""\n
         AUC score: {round(roc_auc_value*100, 2)} %
         Gini index: {round(gini_value*100, 2)} %
         """)

plt.show()

false_positive_rate, true_positive_rate, threshold = roc_curve(y_test,grid_search.predict_proba(X_test_new)[:,1])
roc_auc_value = roc_auc_score(y_test, grid_search.predict_proba(X_test_new)[:,1]).round(4)
gini_value = ((2*roc_auc_value)-1).round(4)

print('AUC for Logistic Regression on val data: ', round(roc_auc_value*100, 2), '%')
print('Gini for Logistic Regression on val data: ', round(gini_value*100, 2), '%')

fig, ax = plt.subplots(1, figsize=(8,6))
plt.title('Receiver Operating Characteristic - LogReg Classifier test')
plt.plot(false_positive_rate, true_positive_rate)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".7"), plt.plot([1, 1] , c=".7")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

plt.text(ax.get_xlim()[1]*6/10, 
         0, 
         f"""\n
         AUC score: {round(roc_auc_value*100, 2)} %
         Gini index: {round(gini_value*100, 2)} %
         """)

plt.show()

y_train_pred = grid_search.predict(X_train_new)
cnf_matrix = metrics.confusion_matrix(y_train, y_train_pred)
print(metrics.classification_report(y_train, y_train_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

y_test_pred = grid_search.predict(X_test_new)
cnf_matrix = metrics.confusion_matrix(y_test, y_test_pred)
print(metrics.classification_report(y_test,y_test_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

"""Dari hasil model, terlihat bahwa Bad borrower masih cukup banyak yang terhitung sebagai good borrowe sehingga kita perlu menurunkan hal ini, dengan cara mencari cut proba terbaik untuk meningkatkan recall dari model

### CUT OFF
"""

data_train = pd.DataFrame()
data_test = pd.DataFrame()

data_train["proba"] = grid_search.predict_proba(X_train_new)[:,1]
data_test["proba"] = grid_search.predict_proba(X_test_new)[:,1]

data_train["label"] = pd.DataFrame(y_train).reset_index(drop=True)
data_test["label"] = pd.DataFrame(y_test).reset_index(drop=True)

data_test_split_proba = data_test[['label', 'proba']]
data_test_split_proba['bins'] = pd.qcut(data_test_split_proba['proba'], q=8)
#data_test_split_proba['bins'] = pd.cut(data_test_split_proba['proba'], bins=[0.010499999999999999, 0.085, 0.188, 0.331, 0.4,0.548, 0.943])
data_test_split_proba.head()

cnt_per_bin2 = data_test_split_proba.bins.value_counts().to_frame().reset_index().rename(columns={'bins':'cnt_debtors'}).sort_values('index')
cnt_per_bin2.head()

bad_per_bin2 = data_test_split_proba.groupby(['bins','label']).count().reset_index().rename(columns={'proba':'cnt_bad_debtors'})
bad_per_bin2 = bad_per_bin2[bad_per_bin2['label']==0]
bad_per_bin2.head()

summary2 = bad_per_bin2.merge(cnt_per_bin2, how='inner', right_on='index', left_on='bins')
summary2['pct_bad_average'] = round((summary2['cnt_bad_debtors'] / summary2['cnt_debtors'])*100 , 2)
summary2.head()

summary2['bins'] = summary2.bins.astype(str)
plt.figure(figsize=(12,9))
sns.lineplot(data = summary2, x='bins', y='pct_bad_average')
plt.xticks(rotation=45)
plt.ylabel('% average_good')
plt.title("Trend Average Bad Borrower Rate tiap Binning untuk Data Test", size=16)
# label points on the plot
for x, y in zip(summary2['bins'], summary2['pct_bad_average']):# the position of the data label relative to the data point can be adjusted by adding/subtracting a value from the x &/ y coordinates
    plt.text(x = x, # x-coordinate position of data label
             y = y+0.001,
             s = '{:.2f}'.format(y), # data label, formatted to ignore decimals
             color = 'black') # set colour of line# y

tr = 0.193
y_train_pred = np.where(grid_search.predict_proba(X_train_new)[:,1]> tr,1,0)
cnf_matrix = metrics.confusion_matrix(y_train, y_train_pred)
print(metrics.classification_report(y_train, y_train_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

tr = 0.392
y_test_pred = np.where(grid_search.predict_proba(X_test_new)[:,1]> tr,1,0)
cnf_matrix = metrics.confusion_matrix(y_test, y_test_pred)
print(metrics.classification_report(y_test,y_test_pred))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

"""dengan mengambil titik cut off proba di 0.132, model telah berhasil menurukan nilai dari bad borrower yang terhitung sebagai good borrower dengan sangat baik. hal ini sangat diperlukan mengingat ketika perusahaan telah memiliki nama yang besar maka akan banyak orang yang akan melakukan peminjaman pada perusahaan tersebut jadi kita memerlukan ML yang sangat baik untuk dapat memisahkan mana peminjam yang **Good** dan **Bad** dengan sangat baik"""

fig, ax = plt.subplots(figsize=(12,3))
sns.histplot(x=grid_search.predict_proba(X_train_new)[:,1],
             binwidth=0.025,
             kde=True,
             ax=ax)

ax.set_title('Logreg Proba Training')
ax.set_xlabel('Prediction Probability')
ax.set_xlim(0,1)

fig, ax = plt.subplots(figsize=(12,3))
sns.histplot(x=grid_search.predict_proba(X_test_new)[:,1],
             binwidth=0.025,
             kde=True,
             ax=ax)

ax.set_title('Logreg Proba Testing')
ax.set_xlabel('Prediction Probability')
ax.set_xlim(0,1)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

rfc_pred_train = rfc.predict(X_train)
rfc_pred_test = rfc.predict(X_test)

# View accuracy score
accuracy_score(y_test, rfc_pred_test)

print(classification_report(y_test,rfc_pred_test))
sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Realita')

false_positive_rate, true_positive_rate, threshold = roc_curve(y_test,rfc.predict_proba(X_test)[:,1])
roc_auc_value = roc_auc_score(y_test, rfc.predict_proba(X_test)[:,1]).round(4)
gini_value = ((2*roc_auc_value)-1).round(4)

print('AUC for Random Forest on val data: ', round(roc_auc_value*100, 2), '%')
print('Gini for Random Forest on val data: ', round(gini_value*100, 2), '%')

fig, ax = plt.subplots(1, figsize=(8,6))
plt.title('Receiver Operating Characteristic - Randfost Classifier test')
plt.plot(false_positive_rate, true_positive_rate)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".7"), plt.plot([1, 1] , c=".7")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

plt.text(ax.get_xlim()[1]*6/10, 
         0, 
         f"""\n
         AUC score: {round(roc_auc_value*100, 2)} %
         Gini index: {round(gini_value*100, 2)} %
         """)

plt.show()

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 5)], # Jumlah subtree 
                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak
                       criterion = ['gini','entropy'],
                       max_depth = [int(x) for x in np.linspace(10, 20, num = 6)],  # Maximum kedalaman tree
                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru
                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru
                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split
                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core
                      )

# Init
rfc = RandomForestClassifier(random_state=42)
rfc_tuned = RandomizedSearchCV(rfc, hyperparameters, cv=5, random_state=42, scoring='roc_auc')
rfc_tuned.fit(X_train,y_train)

# Predict
y_pred_proba_rfc_tuned = rfc_tuned.predict_proba(X_test)
y_pred_test_rfc_tuned = rfc_tuned.predict(X_test)

print('Best n_estimators:', rfc_tuned.best_estimator_.get_params()['n_estimators'])
print('Best bootstrap:', rfc_tuned.best_estimator_.get_params()['bootstrap'])
print('Best criterion:', rfc_tuned.best_estimator_.get_params()['criterion'])
print('Best max_depth:', rfc_tuned.best_estimator_.get_params()['max_depth'])
print('Best min_samples_split:', rfc_tuned.best_estimator_.get_params()['min_samples_split'])
print('Best min_samples_leaf:', rfc_tuned.best_estimator_.get_params()['min_samples_leaf'])
print('Best max_features:', rfc_tuned.best_estimator_.get_params()['max_features'])
print('Best n_jobs:', rfc_tuned.best_estimator_.get_params()['n_jobs'])

Log_ROC_auc = roc_auc_score(y_test, y_pred_test_rfc_tuned)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_rfc_tuned [:, 1])

plt.figure()
plt.plot(fpr, tpr, label = "Random Forest Tuned Model (area = %0.2f)" % Log_ROC_auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

"""## Business Metrics Calculation

### Loss Given Default
"""

DF_TRAIN.shape

DF_TRAIN.head()

DF_TRAIN['AMT_CREDIT'] = scaler_AMT_CREDIT.inverse_transform(DF_TRAIN['AMT_CREDIT'].values.reshape(len(DF_TRAIN), 1))

DF_TRAIN.head()

X = DF_TRAIN.drop(columns = ['TARGET'])
Y = DF_TRAIN[['TARGET']]
XTrain, XTest, yTrain, yTest = train_test_split(X,Y,test_size = 0.2,random_state = 42)

DF_TRAIN.shape

preds_test.shape

XTest.shape

XTest['TARGET_PRED_RESULT'] = preds_test
XTest['TARGET'] = yTest

XTest.shape

XTest.head()

"""### Loss Given Default Before Using Model"""

Defaulters = XTest.groupby(['TARGET'])['AMT_CREDIT'].sum().reset_index()
Defaulters

Total_Defaulters = XTest.groupby(['TARGET'])['AMT_CREDIT'].count().reset_index()
Total_Defaulters

LGDBefore = Defaulters['AMT_CREDIT'].loc[Defaulters['TARGET'] == 1]
LGDBefore

"""### Loss Given Default After Using Model"""

FalseNegative = (XTest.loc[(XTest['TARGET'] == 1) & (XTest['TARGET_PRED_RESULT'] == 0)])

FalseNegative.shape

LGDAfter = FalseNegative['AMT_CREDIT'].sum()
LGDAfter

LGDAfter - LGDBefore

LGD_Decreased_Percentage = ((LGDAfter-LGDBefore)/LGDBefore)*100
LGD_Decreased_Percentage

"""## Predict Data Test Using Model

Untuk melakukan predict data tes kita akan menggunakan feature berdasar IV > 0.01
"""

#select fitur
feature_keep = df_analysis[df_analysis["iv"] > 0.01].index
feature_keep

feature_keep1 = ['EXT_SOURCE_2', 'EXT_SOURCE_3', 'EXT_SOURCE_1', 'AGE', 'YEARS_EMPLOYED',
       'DAYS_LAST_PHONE_CHANGE', 'DAYS_ID_PUBLISH', 'AMT_GOODS_PRICE',
       'REGION_RATING_CLIENT_W_CITY', 'FLOORSMAX_AVG', 'REGION_RATING_CLIENT',
       'FLOORSMAX_MEDI', 'FLOORSMAX_MODE', 'LIVINGAREA_MEDI', 'LIVINGAREA_AVG',
       'TOTALAREA_MODE', 'LIVINGAREA_MODE', 'DAYS_REGISTRATION',
       'ELEVATORS_AVG', 'ELEVATORS_MEDI', 'APARTMENTS_MEDI', 'APARTMENTS_AVG',
       'ELEVATORS_MODE', 'APARTMENTS_MODE', 'YEARS_BEGINEXPLUATATION_MEDI',
       'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BEGINEXPLUATATION_MODE',
       'BASEMENTAREA_AVG', 'ENTRANCES_AVG', 'BASEMENTAREA_MEDI',
       'ENTRANCES_MEDI', 'ENTRANCES_MODE', 'NONLIVINGAREA_MODE',
       'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI', 'BASEMENTAREA_MODE',
       'LANDAREA_AVG', 'LANDAREA_MEDI', 'LANDAREA_MODE']

df_test = df_test[feature_keep1]

logres = LogisticRegression()
logres.fit(X_train, y_train)

y_pred_proba_logres_df_test = logres.predict_proba(df_test)
y_pred_proba_logres_df_test

df_test.shape

apptes.shape

PredictResultTest = apptes['SK_ID_CURR'].reset_index()
PredictResultTest['TARGET'] = round(pd.DataFrame(y_pred_proba_logres_df_test[:,1]),1)
PredictResultTest.drop('index', axis=1, inplace=True)
PredictResultTest.head()

PredictResultTest.shape

PredictResultTest.to_csv("result.txt", sep=',', header=True, index=False)